{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10class_ts_tf1.ipynb",
      "provenance": [],
      "mount_file_id": "19_DX9gH0Gkv-H2Nx5KfoQoU9AjHR563P",
      "authorship_tag": "ABX9TyPDZamK6yXe3RSzBSQHc+S8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjungmo/SBA2020_FinalProject/blob/main/10class_ts_tf1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwCye_X2A-W_"
      },
      "source": [
        "# /content/drive/My Drive/lane/traffic_signs_classification.zip\n",
        "\n",
        "path_to_zip_file = '/content/drive/My Drive/lane/traffic_signs_classification_decr_10.zip'\n",
        "directory_to_extract_to = '/content/trafficsign'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "  zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8dkqh1xBOA5",
        "outputId": "3cdadce2-3e50-49a1-ce00-192f555812bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorflow==1.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1MB 119kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.33.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.8.1)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0) (50.3.2)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.8MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=e148bf42c18a9cd358959456fbfc5f625cf777b6bdda057b0dab1def3ab10e24\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ5LugbaBQUm",
        "outputId": "3517b280-0ddc-43ef-98fa-3f35f4b1cc8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install keras==2.1.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 245kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 256kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 266kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 276kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 286kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 296kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 307kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 317kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 327kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 337kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (3.13)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhyfKWDBRtl",
        "outputId": "6fbfd74c-f49e-49bc-a091-4404f103e36d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "# data_gen 코드 모듈을 제일 먼저 작성한다.\n",
        "\n",
        "\n",
        "def traffic_data_gen(data_path, model_size):  # 데이터셋의 위치를 받고 모델사이즈를 받아서 모델에 넣을 데이터 생성\n",
        "\n",
        "    label = 0\n",
        "    data_set = []\n",
        "    traffic_dict = []\n",
        "    save_flag = 0\n",
        "    count = 0\n",
        "\n",
        "    train_data_path = \"trafficdata/train/\"\n",
        "    test_data_path = \"trafficdata/test/\"\n",
        "    traffic_name_txt = \"traffic_name.txt\"  # 나중에 파일 라벨 확인할 txt 문서 만들어 둔 것.(편의상)\n",
        "\n",
        "    if not os.path.isdir(train_data_path):  # 해당 디렉토리에 없다면 만들어라\n",
        "        os.makedirs(train_data_path)\n",
        "    if not os.path.isdir(test_data_path):\n",
        "        os.makedirs(test_data_path)\n",
        "\n",
        "    if len(glob.glob(train_data_path + \"*.jpg\")) > 10:  # 훈련 데이터 경로의 이미지가 10개를 초과하면 save_flag= 1\n",
        "        save_flag = 1  # 즉 이미 있으니 저장 안한다는 말. 읽어오기만 해도 된다.\n",
        "\n",
        "    # 데이터가 없다면\n",
        "    # 훈련 데이터 경로에 이미지가 없다면( 9개 이하라면)\n",
        "    if save_flag == 0:\n",
        "\n",
        "        fld_list = glob.glob(data_path + '/*')  # 폴더의 리스트를 가져온다. 데이터 경로의 모든 폴더들\n",
        "        for i in tqdm(fld_list):  # 여기서 i는 폴더리스트에 있는 요소 하나다. 즉 폴더 하나가 라벨 하나다.\n",
        "            img_list = i + '/*.jpg'  # img_list는 폴더리스트에 들어간 이미지들의 리스트경로를 가져오게 하는  값 설정\n",
        "            img_list = glob.glob(img_list)  # 해당 리스트들을 모두 파일로 가져온다.\n",
        "            for j in img_list:  # j는 폴더안의 이미지 하나 하나\n",
        "                img = cv2.imread(j)  # 이미지를 읽어서\n",
        "                # img = cv2.resize(img, (model_size, model_size))  # resize해주고\n",
        "                traffic_name = i.replace('/content/trafficsign/traffic_signs_classification/myData', '~~')   # 폴더명별로 되어있는 라벨 이름만 남김.\n",
        "                data_set.append([img, label])  # 이미지와 그에따른 라벨을 데이터셋에 추가한다.\n",
        "            traffic_dict.append(traffic_name)  # 라벨 사전에는 j의 포문 후 라벨 이름을 추가한다.\n",
        "            label += 1\n",
        "#                 img_list 포문이 돌때마다 라벨별 폴더의 이미지를 읽어서 라벨링해준다.\n",
        "#                     그에 맞는 라벨 이름도 라벨 사전에 추가된다.\n",
        "\n",
        "\n",
        "        with open(traffic_name_txt, 'w') as txt:\n",
        "            for n in traffic_dict:  # 라벨 사전에 적힌 이름들( 폴더명) 개수 만큼 돌면서\n",
        "                txt.writelines(str(n))  # txt파일에 이름을 쭉 write한다.\n",
        "                txt.writelines(\" \")  # 한칸씩 띄어쓰면서\n",
        " \n",
        "        indexer = int(len(data_set) * 0.8)  # 훈련/시험 데이터 나누는 비율\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        x_test = []\n",
        "        y_test = []\n",
        "\n",
        "        random.shuffle(data_set)  # 전체 데이터 섞어줌\n",
        "\n",
        "        count = 0\n",
        "        for i in tqdm(range(0, indexer)):  # 훈련데이터\n",
        "            x_train.append(data_set[i][0])\n",
        "            y_train.append(data_set[i][1])  # 위에서 [img, label]로 append해서 이렇게 [i][0] , [i][1]\n",
        "\n",
        "            save_img_path = train_data_path + \"%d.jpg\" % count  # 이미지 경로\n",
        "            save_label_file = train_data_path + \"%d.txt\" % count  # 라벨(텍스트) 경로\n",
        "\n",
        "            cv2.imwrite(save_img_path, data_set[i][0])  # 이미지 저장\n",
        "            with open(save_label_file, 'w') as txt:  # 텍스트 저장 라벨링 한 숫자에 맞아떨어지게\n",
        "                txt.writelines(str(data_set[i][1]))\n",
        "            count += 1\n",
        "\n",
        "        count = 0  # 시험 데이터도 돌려야 해서 local변수 선언 해준 것이다. 위의 count와 영향 없게\n",
        "        for i in tqdm(range(indexer, len(data_set))):\n",
        "            x_test.append(data_set[i][0])\n",
        "            y_test.append(data_set[i][1])\n",
        "\n",
        "            save_img_path = test_data_path + \"%d.jpg\" % count  # 이미지 경로\n",
        "            save_label_file = test_data_path + \"%d.txt\" % count  # 라벨(텍스트) 경로\n",
        "\n",
        "            cv2.imwrite(save_img_path, data_set[i][0])  # 이미지 저장\n",
        "\n",
        "            with open(save_label_file, 'w') as txt:  # 텍스트 저장\n",
        "                txt.writelines(str(data_set[i][1]))\n",
        "            count += 1\n",
        "\n",
        "        return x_train, y_train, x_test, y_test, traffic_dict\n",
        "\n",
        "    elif save_flag == 1:  # 이미 파일이 준비되어 있는 경우\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        x_test = []\n",
        "        y_test = []\n",
        "\n",
        "        train_data_list = glob.glob(train_data_path + \"*.jpg\")  # 데이터가 이미 존재하니까. 이미지 불러옴\n",
        "        test_data_list = glob.glob(test_data_path + \"*.jpg\")\n",
        "\n",
        "\n",
        "        for i in tqdm(range(0, len(train_data_list))):  # 이미지 하나하나 for문돌리면서\n",
        "            file = train_data_list[i]  # i번째 이미지\n",
        "            img = cv2.imread(file)  # 읽고\n",
        "            label_path = file.replace(\".jpg\", \".txt\")  # 라벨 데이터 읽기 위해 path만듬\n",
        "            with open(label_path, 'r') as txt:  # 라벨 데이터 txt안에서 라벨 읽어옴\n",
        "                label = txt.readlines()\n",
        "            label = int(label[0])  # 각 이미지 별 라벨 값을 얻는다.\n",
        "            x_train.append(img)  # 이미지는 x에\n",
        "            y_train.append(label)  # 라벨은 y에 넣는다\n",
        "        # 테스트 데이터도 마찬가지\n",
        "        for i in tqdm(range(0, len(test_data_list))):\n",
        "            file = test_data_list[i]\n",
        "            img = cv2.imread(file)\n",
        "            label_path = file.replace(\".jpg\", \".txt\")\n",
        "            with open(label_path, 'r') as txt:\n",
        "                label = txt.readlines()\n",
        "            label = int(label[0])\n",
        "            x_test.append(img)\n",
        "            y_test.append(label)\n",
        "        # 라벨 이름을 불러오기 위해 만들어둔 딕셔너리에서 찾아오는 방법\n",
        "        with open(traffic_name_txt, 'r') as txt:\n",
        "            label = txt.readlines()\n",
        "            traffic_dict = label[0].split(\"~~\")  # 만든 파일이 모두 띄어쓰기로 이루어진것\n",
        "\n",
        "        return x_train, y_train, x_test, y_test, traffic_dict\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/trafficsign/traffic_signs_classification/myData\"\n",
        "    model_size = 32\n",
        "    x_train, y_train, x_test, y_test, traffic_dict = traffic_data_gen(data_path, model_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11231/11231 [00:01<00:00, 10263.83it/s]\n",
            "100%|██████████| 2808/2808 [00:00<00:00, 10419.09it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3p2pMWZBX_D",
        "outputId": "2386b770-1680-49e6-a926-c137bd8b6585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_10_decr_1\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(16, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(128, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "x= keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(10)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(11231, 32, 32, 3) (11231, 10) (2808, 32, 32, 3) (2808, 10)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 128)         18560     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 53,518\n",
            "Trainable params: 53,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11231 samples, validate on 2808 samples\n",
            "Epoch 1/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 1.2408 - acc: 0.5971 - val_loss: 0.4275 - val_acc: 0.8768\n",
            "Epoch 2/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 0.2757 - acc: 0.9157 - val_loss: 0.1895 - val_acc: 0.9516\n",
            "Epoch 3/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 0.1143 - acc: 0.9706 - val_loss: 0.1084 - val_acc: 0.9726\n",
            "Epoch 4/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 0.0601 - acc: 0.9831 - val_loss: 0.1102 - val_acc: 0.9726\n",
            "Epoch 5/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 0.0431 - acc: 0.9877 - val_loss: 0.0727 - val_acc: 0.9847\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_10_decr_1-00005.h5\n",
            "Epoch 6/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.0328 - val_acc: 0.9922\n",
            "Epoch 7/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 0.0284 - acc: 0.9920 - val_loss: 0.0438 - val_acc: 0.9904\n",
            "Epoch 8/10\n",
            "11231/11231 [==============================] - 14s 1ms/step - loss: 0.0270 - acc: 0.9934 - val_loss: 0.0505 - val_acc: 0.9890\n",
            "Epoch 9/10\n",
            "11231/11231 [==============================] - 15s 1ms/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0642 - val_acc: 0.9868\n",
            "Epoch 10/10\n",
            "11231/11231 [==============================] - 17s 2ms/step - loss: 0.0222 - acc: 0.9936 - val_loss: 0.0524 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_10_decr_1-00010.h5\n",
            "Acc: 98.96723646723646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcUwcCNRB4rw",
        "outputId": "abfe5076-b032-452f-87a0-d5b39eb4b383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_10_decr_2\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(16, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "x= keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(10)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11231, 32, 32, 3) (11231, 10) (2808, 32, 32, 3) (2808, 10)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,574\n",
            "Trainable params: 18,574\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11231 samples, validate on 2808 samples\n",
            "Epoch 1/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 1.3785 - acc: 0.5609 - val_loss: 0.5297 - val_acc: 0.8522\n",
            "Epoch 2/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.3425 - acc: 0.9041 - val_loss: 0.2041 - val_acc: 0.9519\n",
            "Epoch 3/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.1485 - acc: 0.9618 - val_loss: 0.1258 - val_acc: 0.9715\n",
            "Epoch 4/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.0805 - acc: 0.9803 - val_loss: 0.0727 - val_acc: 0.9811\n",
            "Epoch 5/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.0634 - acc: 0.9830 - val_loss: 0.0708 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_10_decr_2-00005.h5\n",
            "Epoch 6/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.0385 - acc: 0.9908 - val_loss: 0.0375 - val_acc: 0.9915\n",
            "Epoch 7/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.0277 - acc: 0.9940 - val_loss: 0.0270 - val_acc: 0.9947\n",
            "Epoch 8/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.0131 - acc: 0.9979 - val_loss: 0.0367 - val_acc: 0.9929\n",
            "Epoch 9/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.0272 - acc: 0.9932 - val_loss: 0.0274 - val_acc: 0.9954\n",
            "Epoch 10/10\n",
            "11231/11231 [==============================] - 13s 1ms/step - loss: 0.0135 - acc: 0.9976 - val_loss: 0.0245 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_10_decr_2-00010.h5\n",
            "Acc: 99.60826210826211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0l7uJ-pCxiZ",
        "outputId": "d5a7c3dc-c3c9-4c4b-ea6a-5117f728cee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_10_decr_3\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(10)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11231, 32, 32, 3) (11231, 10) (2808, 32, 32, 3) (2808, 10)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 5,438\n",
            "Trainable params: 5,438\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11231 samples, validate on 2808 samples\n",
            "Epoch 1/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 1.1402 - acc: 0.6487 - val_loss: 0.3550 - val_acc: 0.9192\n",
            "Epoch 2/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.2233 - acc: 0.9485 - val_loss: 0.1654 - val_acc: 0.9605\n",
            "Epoch 3/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.1193 - acc: 0.9752 - val_loss: 0.1023 - val_acc: 0.9801\n",
            "Epoch 4/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.0774 - acc: 0.9874 - val_loss: 0.0729 - val_acc: 0.9875\n",
            "Epoch 5/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.0566 - acc: 0.9914 - val_loss: 0.0590 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_10_decr_3-00005.h5\n",
            "Epoch 6/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.0477 - acc: 0.9914 - val_loss: 0.0434 - val_acc: 0.9915\n",
            "Epoch 7/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.0317 - acc: 0.9960 - val_loss: 0.0321 - val_acc: 0.9936\n",
            "Epoch 8/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.0253 - acc: 0.9972 - val_loss: 0.0268 - val_acc: 0.9943\n",
            "Epoch 9/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.0274 - acc: 0.9947 - val_loss: 0.0301 - val_acc: 0.9922\n",
            "Epoch 10/10\n",
            "11231/11231 [==============================] - 12s 1ms/step - loss: 0.0194 - acc: 0.9972 - val_loss: 0.0282 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_10_decr_3-00010.h5\n",
            "Acc: 99.25213675213675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tspRGYCBCy03",
        "outputId": "0a1ca22f-d1d4-48fd-e16a-ae5ad9360e23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_10_decr_4\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(10)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11231, 32, 32, 3) (11231, 10) (2808, 32, 32, 3) (2808, 10)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                7690      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 7,774\n",
            "Trainable params: 7,774\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11231 samples, validate on 2808 samples\n",
            "Epoch 1/10\n",
            "11231/11231 [==============================] - 9s 817us/step - loss: 1.1043 - acc: 0.6981 - val_loss: 0.4597 - val_acc: 0.8917\n",
            "Epoch 2/10\n",
            "11231/11231 [==============================] - 9s 795us/step - loss: 0.3231 - acc: 0.9194 - val_loss: 0.2741 - val_acc: 0.9295\n",
            "Epoch 3/10\n",
            "11231/11231 [==============================] - 9s 801us/step - loss: 0.2037 - acc: 0.9492 - val_loss: 0.1762 - val_acc: 0.9541\n",
            "Epoch 4/10\n",
            "11231/11231 [==============================] - 9s 788us/step - loss: 0.1719 - acc: 0.9559 - val_loss: 0.1436 - val_acc: 0.9747\n",
            "Epoch 5/10\n",
            "11231/11231 [==============================] - 9s 795us/step - loss: 0.1441 - acc: 0.9680 - val_loss: 0.1242 - val_acc: 0.9747\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_10_decr_4-00005.h5\n",
            "Epoch 6/10\n",
            "11231/11231 [==============================] - 9s 795us/step - loss: 0.1136 - acc: 0.9747 - val_loss: 0.1554 - val_acc: 0.9533\n",
            "Epoch 7/10\n",
            "11231/11231 [==============================] - 9s 800us/step - loss: 0.1155 - acc: 0.9734 - val_loss: 0.1035 - val_acc: 0.9747\n",
            "Epoch 8/10\n",
            "11231/11231 [==============================] - 9s 804us/step - loss: 0.1021 - acc: 0.9764 - val_loss: 0.0822 - val_acc: 0.9833\n",
            "Epoch 9/10\n",
            "11231/11231 [==============================] - 9s 791us/step - loss: 0.0918 - acc: 0.9794 - val_loss: 0.0801 - val_acc: 0.9825\n",
            "Epoch 10/10\n",
            "11231/11231 [==============================] - 9s 792us/step - loss: 0.0841 - acc: 0.9826 - val_loss: 0.0797 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_10_decr_4-00010.h5\n",
            "Acc: 98.25498573800438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7GU62LYGSFS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
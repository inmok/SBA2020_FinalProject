{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14class_ts_tf1.ipynb",
      "provenance": [],
      "mount_file_id": "1Bkn4tLJSrDXusMZ5ceEHT80rHlxuuoLO",
      "authorship_tag": "ABX9TyOtOsRDSxw4+3WwVby8qN5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjungmo/SBA2020_FinalProject/blob/main/14class_ts_tf1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwCye_X2A-W_"
      },
      "source": [
        "# /content/drive/My Drive/lane/traffic_signs_classification.zip\n",
        "\n",
        "path_to_zip_file = '/content/drive/My Drive/lane/traffic_signs_classification_decreased.zip'\n",
        "directory_to_extract_to = '/content/trafficsign'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "  zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8dkqh1xBOA5",
        "outputId": "d3e75cff-5535-4ead-e33c-df409da030ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorflow==1.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1MB 65kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.33.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.35.1)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (3.12.4)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0) (50.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=323356444747b8f182e7bb213ad0b77190b20d98a07ee72ce4cce66b777699fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ5LugbaBQUm",
        "outputId": "22ee5954-2c5b-41da-9397-79a35e5232ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install keras==2.1.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 71kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 81kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 174kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 184kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 194kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 204kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 215kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 225kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 235kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 245kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 256kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 266kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 276kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 286kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 296kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 307kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 317kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 327kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 337kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (2.10.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhyfKWDBRtl",
        "outputId": "a4e2728d-ef3c-4811-c36d-0272fac007b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "# data_gen 코드 모듈을 제일 먼저 작성한다.\n",
        "\n",
        "\n",
        "def traffic_data_gen(data_path, model_size):  # 데이터셋의 위치를 받고 모델사이즈를 받아서 모델에 넣을 데이터 생성\n",
        "\n",
        "    label = 0\n",
        "    data_set = []\n",
        "    traffic_dict = []\n",
        "    save_flag = 0\n",
        "    count = 0\n",
        "\n",
        "    train_data_path = \"trafficdata/train/\"\n",
        "    test_data_path = \"trafficdata/test/\"\n",
        "    traffic_name_txt = \"traffic_name.txt\"  # 나중에 파일 라벨 확인할 txt 문서 만들어 둔 것.(편의상)\n",
        "\n",
        "    if not os.path.isdir(train_data_path):  # 해당 디렉토리에 없다면 만들어라\n",
        "        os.makedirs(train_data_path)\n",
        "    if not os.path.isdir(test_data_path):\n",
        "        os.makedirs(test_data_path)\n",
        "\n",
        "    if len(glob.glob(train_data_path + \"*.jpg\")) > 10:  # 훈련 데이터 경로의 이미지가 10개를 초과하면 save_flag= 1\n",
        "        save_flag = 1  # 즉 이미 있으니 저장 안한다는 말. 읽어오기만 해도 된다.\n",
        "\n",
        "    # 데이터가 없다면\n",
        "    # 훈련 데이터 경로에 이미지가 없다면( 9개 이하라면)\n",
        "    if save_flag == 0:\n",
        "\n",
        "        fld_list = glob.glob(data_path + '/*')  # 폴더의 리스트를 가져온다. 데이터 경로의 모든 폴더들\n",
        "        for i in tqdm(fld_list):  # 여기서 i는 폴더리스트에 있는 요소 하나다. 즉 폴더 하나가 라벨 하나다.\n",
        "            img_list = i + '/*.jpg'  # img_list는 폴더리스트에 들어간 이미지들의 리스트경로를 가져오게 하는  값 설정\n",
        "            img_list = glob.glob(img_list)  # 해당 리스트들을 모두 파일로 가져온다.\n",
        "            for j in img_list:  # j는 폴더안의 이미지 하나 하나\n",
        "                img = cv2.imread(j)  # 이미지를 읽어서\n",
        "                # img = cv2.resize(img, (model_size, model_size))  # resize해주고\n",
        "                traffic_name = i.replace('/content/trafficsign/traffic_signs_classification/myData', '~~')   # 폴더명별로 되어있는 라벨 이름만 남김.\n",
        "                data_set.append([img, label])  # 이미지와 그에따른 라벨을 데이터셋에 추가한다.\n",
        "            traffic_dict.append(traffic_name)  # 라벨 사전에는 j의 포문 후 라벨 이름을 추가한다.\n",
        "            label += 1\n",
        "#                 img_list 포문이 돌때마다 라벨별 폴더의 이미지를 읽어서 라벨링해준다.\n",
        "#                     그에 맞는 라벨 이름도 라벨 사전에 추가된다.\n",
        "\n",
        "\n",
        "        with open(traffic_name_txt, 'w') as txt:\n",
        "            for n in traffic_dict:  # 라벨 사전에 적힌 이름들( 폴더명) 개수 만큼 돌면서\n",
        "                txt.writelines(str(n))  # txt파일에 이름을 쭉 write한다.\n",
        "                txt.writelines(\" \")  # 한칸씩 띄어쓰면서\n",
        " \n",
        "        indexer = int(len(data_set) * 0.8)  # 훈련/시험 데이터 나누는 비율\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        x_test = []\n",
        "        y_test = []\n",
        "\n",
        "        random.shuffle(data_set)  # 전체 데이터 섞어줌\n",
        "\n",
        "        count = 0\n",
        "        for i in tqdm(range(0, indexer)):  # 훈련데이터\n",
        "            x_train.append(data_set[i][0])\n",
        "            y_train.append(data_set[i][1])  # 위에서 [img, label]로 append해서 이렇게 [i][0] , [i][1]\n",
        "\n",
        "            save_img_path = train_data_path + \"%d.jpg\" % count  # 이미지 경로\n",
        "            save_label_file = train_data_path + \"%d.txt\" % count  # 라벨(텍스트) 경로\n",
        "\n",
        "            cv2.imwrite(save_img_path, data_set[i][0])  # 이미지 저장\n",
        "            with open(save_label_file, 'w') as txt:  # 텍스트 저장 라벨링 한 숫자에 맞아떨어지게\n",
        "                txt.writelines(str(data_set[i][1]))\n",
        "            count += 1\n",
        "\n",
        "        count = 0  # 시험 데이터도 돌려야 해서 local변수 선언 해준 것이다. 위의 count와 영향 없게\n",
        "        for i in tqdm(range(indexer, len(data_set))):\n",
        "            x_test.append(data_set[i][0])\n",
        "            y_test.append(data_set[i][1])\n",
        "\n",
        "            save_img_path = test_data_path + \"%d.jpg\" % count  # 이미지 경로\n",
        "            save_label_file = test_data_path + \"%d.txt\" % count  # 라벨(텍스트) 경로\n",
        "\n",
        "            cv2.imwrite(save_img_path, data_set[i][0])  # 이미지 저장\n",
        "\n",
        "            with open(save_label_file, 'w') as txt:  # 텍스트 저장\n",
        "                txt.writelines(str(data_set[i][1]))\n",
        "            count += 1\n",
        "\n",
        "        return x_train, y_train, x_test, y_test, traffic_dict\n",
        "\n",
        "    elif save_flag == 1:  # 이미 파일이 준비되어 있는 경우\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        x_test = []\n",
        "        y_test = []\n",
        "\n",
        "        train_data_list = glob.glob(train_data_path + \"*.jpg\")  # 데이터가 이미 존재하니까. 이미지 불러옴\n",
        "        test_data_list = glob.glob(test_data_path + \"*.jpg\")\n",
        "\n",
        "\n",
        "        for i in tqdm(range(0, len(train_data_list))):  # 이미지 하나하나 for문돌리면서\n",
        "            file = train_data_list[i]  # i번째 이미지\n",
        "            img = cv2.imread(file)  # 읽고\n",
        "            label_path = file.replace(\".jpg\", \".txt\")  # 라벨 데이터 읽기 위해 path만듬\n",
        "            with open(label_path, 'r') as txt:  # 라벨 데이터 txt안에서 라벨 읽어옴\n",
        "                label = txt.readlines()\n",
        "            label = int(label[0])  # 각 이미지 별 라벨 값을 얻는다.\n",
        "            x_train.append(img)  # 이미지는 x에\n",
        "            y_train.append(label)  # 라벨은 y에 넣는다\n",
        "        # 테스트 데이터도 마찬가지\n",
        "        for i in tqdm(range(0, len(test_data_list))):\n",
        "            file = test_data_list[i]\n",
        "            img = cv2.imread(file)\n",
        "            label_path = file.replace(\".jpg\", \".txt\")\n",
        "            with open(label_path, 'r') as txt:\n",
        "                label = txt.readlines()\n",
        "            label = int(label[0])\n",
        "            x_test.append(img)\n",
        "            y_test.append(label)\n",
        "        # 라벨 이름을 불러오기 위해 만들어둔 딕셔너리에서 찾아오는 방법\n",
        "        with open(traffic_name_txt, 'r') as txt:\n",
        "            label = txt.readlines()\n",
        "            traffic_dict = label[0].split(\"~~\")  # 만든 파일이 모두 띄어쓰기로 이루어진것\n",
        "\n",
        "        return x_train, y_train, x_test, y_test, traffic_dict\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/trafficsign/traffic_signs_classification/myData\"\n",
        "    model_size = 32\n",
        "    x_train, y_train, x_test, y_test, traffic_dict = traffic_data_gen(data_path, model_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17831/17831 [00:01<00:00, 10370.34it/s]\n",
            "100%|██████████| 4458/4458 [00:00<00:00, 10176.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3p2pMWZBX_D",
        "outputId": "5d221fbc-7b1a-439c-8f61-91f2f7205d55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_1_decr\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 14)\n",
        "y_test = keras.utils.to_categorical(y_test, 14)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(16, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(128, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "x= keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(14)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17831, 32, 32, 3) (17831, 14) (4458, 32, 32, 3) (4458, 14)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 4, 4, 128)         18560     \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 14)                910       \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 14)                0         \n",
            "=================================================================\n",
            "Total params: 53,778\n",
            "Trainable params: 53,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 17831 samples, validate on 4458 samples\n",
            "Epoch 1/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 1.1239 - acc: 0.6459 - val_loss: 0.3872 - val_acc: 0.8809\n",
            "Epoch 2/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.2203 - acc: 0.9358 - val_loss: 0.1699 - val_acc: 0.9468\n",
            "Epoch 3/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.1097 - acc: 0.9696 - val_loss: 0.0740 - val_acc: 0.9834\n",
            "Epoch 4/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.0688 - acc: 0.9804 - val_loss: 0.0695 - val_acc: 0.9836\n",
            "Epoch 5/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.0487 - acc: 0.9868 - val_loss: 0.0593 - val_acc: 0.9879\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_1_decr-00005.h5\n",
            "Epoch 6/10\n",
            "17831/17831 [==============================] - 22s 1ms/step - loss: 0.0338 - acc: 0.9908 - val_loss: 0.0671 - val_acc: 0.9836\n",
            "Epoch 7/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.0320 - acc: 0.9915 - val_loss: 0.0384 - val_acc: 0.9926\n",
            "Epoch 8/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.0241 - acc: 0.9934 - val_loss: 0.0630 - val_acc: 0.9863\n",
            "Epoch 9/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.0186 - acc: 0.9948 - val_loss: 0.0394 - val_acc: 0.9921\n",
            "Epoch 10/10\n",
            "17831/17831 [==============================] - 23s 1ms/step - loss: 0.0096 - acc: 0.9978 - val_loss: 0.1944 - val_acc: 0.9520\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_1_decr-00010.h5\n",
            "Acc: 95.19964109466127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcUwcCNRB4rw",
        "outputId": "c6096d8e-5556-48c7-85c6-0d37de012aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_2_decr\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 14)\n",
        "y_test = keras.utils.to_categorical(y_test, 14)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(16, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "x= keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(14)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17831, 32, 32, 3) (17831, 14) (4458, 32, 32, 3) (4458, 14)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 14)                910       \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 14)                0         \n",
            "=================================================================\n",
            "Total params: 18,834\n",
            "Trainable params: 18,834\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 17831 samples, validate on 4458 samples\n",
            "Epoch 1/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 1.1818 - acc: 0.6414 - val_loss: 0.3971 - val_acc: 0.8863\n",
            "Epoch 2/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.2403 - acc: 0.9343 - val_loss: 0.1719 - val_acc: 0.9587\n",
            "Epoch 3/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.1241 - acc: 0.9685 - val_loss: 0.1327 - val_acc: 0.9652\n",
            "Epoch 4/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.0833 - acc: 0.9785 - val_loss: 0.0880 - val_acc: 0.9825\n",
            "Epoch 5/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.0540 - acc: 0.9864 - val_loss: 0.0894 - val_acc: 0.9747\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_2_decr-00005.h5\n",
            "Epoch 6/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.0381 - acc: 0.9900 - val_loss: 0.0786 - val_acc: 0.9827\n",
            "Epoch 7/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.0389 - acc: 0.9892 - val_loss: 0.0500 - val_acc: 0.9910\n",
            "Epoch 8/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.0620 - val_acc: 0.9888\n",
            "Epoch 9/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.0231 - acc: 0.9939 - val_loss: 0.0489 - val_acc: 0.9917\n",
            "Epoch 10/10\n",
            "17831/17831 [==============================] - 20s 1ms/step - loss: 0.0124 - acc: 0.9966 - val_loss: 0.0505 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_2_decr-00010.h5\n",
            "Acc: 99.32705248990578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0l7uJ-pCxiZ",
        "outputId": "23c75341-9465-45d7-98f6-9702c0a3f320",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_3_decr\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 14)\n",
        "y_test = keras.utils.to_categorical(y_test, 14)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(14)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17831, 32, 32, 3) (17831, 14) (4458, 32, 32, 3) (4458, 14)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 14)                7182      \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 14)                0         \n",
            "=================================================================\n",
            "Total params: 7,490\n",
            "Trainable params: 7,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 17831 samples, validate on 4458 samples\n",
            "Epoch 1/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 1.1262 - acc: 0.6803 - val_loss: 0.3191 - val_acc: 0.9298\n",
            "Epoch 2/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.2071 - acc: 0.9570 - val_loss: 0.1455 - val_acc: 0.9708\n",
            "Epoch 3/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.1065 - acc: 0.9791 - val_loss: 0.1156 - val_acc: 0.9715\n",
            "Epoch 4/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.0669 - acc: 0.9883 - val_loss: 0.0673 - val_acc: 0.9861\n",
            "Epoch 5/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.0504 - acc: 0.9901 - val_loss: 0.0664 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_3_decr-00005.h5\n",
            "Epoch 6/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.0339 - acc: 0.9939 - val_loss: 0.0671 - val_acc: 0.9843\n",
            "Epoch 7/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.0290 - acc: 0.9950 - val_loss: 0.0496 - val_acc: 0.9892\n",
            "Epoch 8/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.0262 - acc: 0.9945 - val_loss: 0.0784 - val_acc: 0.9805\n",
            "Epoch 9/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.0215 - acc: 0.9956 - val_loss: 0.0449 - val_acc: 0.9913\n",
            "Epoch 10/10\n",
            "17831/17831 [==============================] - 19s 1ms/step - loss: 0.0158 - acc: 0.9967 - val_loss: 0.0427 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_3_decr-00010.h5\n",
            "Acc: 99.28218932256617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tspRGYCBCy03",
        "outputId": "2723330d-4260-415d-842e-1fa0a3e38cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_4_decr\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 14)\n",
        "y_test = keras.utils.to_categorical(y_test, 14)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(14)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17831, 32, 32, 3) (17831, 14) (4458, 32, 32, 3) (4458, 14)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 14)                10766     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 14)                0         \n",
            "=================================================================\n",
            "Total params: 10,850\n",
            "Trainable params: 10,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 17831 samples, validate on 4458 samples\n",
            "Epoch 1/10\n",
            "17831/17831 [==============================] - 15s 819us/step - loss: 1.2775 - acc: 0.6682 - val_loss: 0.6199 - val_acc: 0.8627\n",
            "Epoch 2/10\n",
            "17831/17831 [==============================] - 14s 809us/step - loss: 0.4201 - acc: 0.9076 - val_loss: 0.3499 - val_acc: 0.9170\n",
            "Epoch 3/10\n",
            "17831/17831 [==============================] - 14s 806us/step - loss: 0.2564 - acc: 0.9461 - val_loss: 0.2141 - val_acc: 0.9475\n",
            "Epoch 4/10\n",
            "17831/17831 [==============================] - 14s 807us/step - loss: 0.1868 - acc: 0.9593 - val_loss: 0.2047 - val_acc: 0.9542\n",
            "Epoch 5/10\n",
            "17831/17831 [==============================] - 14s 810us/step - loss: 0.1448 - acc: 0.9705 - val_loss: 0.1453 - val_acc: 0.9681\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_4_decr-00005.h5\n",
            "Epoch 6/10\n",
            "17831/17831 [==============================] - 14s 809us/step - loss: 0.1195 - acc: 0.9745 - val_loss: 0.1388 - val_acc: 0.9670\n",
            "Epoch 7/10\n",
            "17831/17831 [==============================] - 14s 808us/step - loss: 0.0985 - acc: 0.9792 - val_loss: 0.1335 - val_acc: 0.9690\n",
            "Epoch 8/10\n",
            "17831/17831 [==============================] - 14s 813us/step - loss: 0.0888 - acc: 0.9810 - val_loss: 0.0929 - val_acc: 0.9800\n",
            "Epoch 9/10\n",
            "17831/17831 [==============================] - 15s 816us/step - loss: 0.0721 - acc: 0.9861 - val_loss: 0.0871 - val_acc: 0.9803\n",
            "Epoch 10/10\n",
            "17831/17831 [==============================] - 14s 809us/step - loss: 0.0647 - acc: 0.9863 - val_loss: 0.0823 - val_acc: 0.9838\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_4_decr-00010.h5\n",
            "Acc: 98.38492597577388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7GU62LYGSFS",
        "outputId": "a37b0279-67a4-4ee6-fc68-060b76a73eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "import os\n",
        "import colab_utils.tboard\n",
        "\n",
        "# set paths\n",
        "ROOT = %pwd\n",
        "LOG_DIR = os.path.join(ROOT, 'log')\n",
        "\n",
        "# will install `ngrok`, if necessary\n",
        "# will create `log_dir` if path does not exist\n",
        "colab_utils.tboard.launch_tensorboard( bin_dir=ROOT, log_dir=LOG_DIR )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1f19b859a39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcolab_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# set paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pwd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'colab_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlB60wkFKVvX",
        "outputId": "3b1f8957-9428-4a1a-90ac-14f892b5e0ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmnKrzf0KZJj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1RcDL6MZjy5iS06or89LyzAShVGd4FYx9",
      "authorship_tag": "ABX9TyMwC0oUZ5fcu1+L6Z9hHUxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjungmo/SBA2020_FinalProject/blob/main/tf1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axDY8Y4g_K3v"
      },
      "source": [
        "# /content/drive/My Drive/lane/traffic_signs_classification.zip\n",
        "\n",
        "path_to_zip_file = '/content/drive/My Drive/lane/traffic_sign_classification.zip'\n",
        "directory_to_extract_to = '/content/trafficsign'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "  zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poddV64G_OiW",
        "outputId": "4d8c5eab-b065-4a99-b6a8-ccce58d9339a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "# data_gen 코드 모듈을 제일 먼저 작성한다.\n",
        "\n",
        "\n",
        "def traffic_data_gen(data_path, model_size):  # 데이터셋의 위치를 받고 모델사이즈를 받아서 모델에 넣을 데이터 생성\n",
        "\n",
        "    label = 0\n",
        "    data_set = []\n",
        "    traffic_dict = []\n",
        "    save_flag = 0\n",
        "    count = 0\n",
        "\n",
        "    train_data_path = \"trafficdata/train/\"\n",
        "    test_data_path = \"trafficdata/test/\"\n",
        "    traffic_name_txt = \"traffic_name.txt\"  # 나중에 파일 라벨 확인할 txt 문서 만들어 둔 것.(편의상)\n",
        "\n",
        "    if not os.path.isdir(train_data_path):  # 해당 디렉토리에 없다면 만들어라\n",
        "        os.makedirs(train_data_path)\n",
        "    if not os.path.isdir(test_data_path):\n",
        "        os.makedirs(test_data_path)\n",
        "\n",
        "    if len(glob.glob(train_data_path + \"*.jpg\")) > 10:  # 훈련 데이터 경로의 이미지가 10개를 초과하면 save_flag= 1\n",
        "        save_flag = 1  # 즉 이미 있으니 저장 안한다는 말. 읽어오기만 해도 된다.\n",
        "\n",
        "    # 데이터가 없다면\n",
        "    # 훈련 데이터 경로에 이미지가 없다면( 9개 이하라면)\n",
        "    if save_flag == 0:\n",
        "\n",
        "        fld_list = glob.glob(data_path + '/*')  # 폴더의 리스트를 가져온다. 데이터 경로의 모든 폴더들\n",
        "        for i in tqdm(fld_list):  # 여기서 i는 폴더리스트에 있는 요소 하나다. 즉 폴더 하나가 라벨 하나다.\n",
        "            img_list = i + '/*.jpg'  # img_list는 폴더리스트에 들어간 이미지들의 리스트경로를 가져오게 하는  값 설정\n",
        "            img_list = glob.glob(img_list)  # 해당 리스트들을 모두 파일로 가져온다.\n",
        "            for j in img_list:  # j는 폴더안의 이미지 하나 하나\n",
        "                img = cv2.imread(j)  # 이미지를 읽어서\n",
        "                # img = cv2.resize(img, (model_size, model_size))  # resize해주고\n",
        "                traffic_name = i.replace('/content/trafficsign/traffic_signs_classification/myData', '~~')   # 폴더명별로 되어있는 라벨 이름만 남김.\n",
        "                data_set.append([img, label])  # 이미지와 그에따른 라벨을 데이터셋에 추가한다.\n",
        "            traffic_dict.append(traffic_name)  # 라벨 사전에는 j의 포문 후 라벨 이름을 추가한다.\n",
        "            label += 1\n",
        "#                 img_list 포문이 돌때마다 라벨별 폴더의 이미지를 읽어서 라벨링해준다.\n",
        "#                     그에 맞는 라벨 이름도 라벨 사전에 추가된다.\n",
        "\n",
        "\n",
        "        with open(traffic_name_txt, 'w') as txt:\n",
        "            for n in traffic_dict:  # 라벨 사전에 적힌 이름들( 폴더명) 개수 만큼 돌면서\n",
        "                txt.writelines(str(n))  # txt파일에 이름을 쭉 write한다.\n",
        "                txt.writelines(\" \")  # 한칸씩 띄어쓰면서\n",
        " \n",
        "        indexer = int(len(data_set) * 0.8)  # 훈련/시험 데이터 나누는 비율\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        x_test = []\n",
        "        y_test = []\n",
        "\n",
        "        random.shuffle(data_set)  # 전체 데이터 섞어줌\n",
        "\n",
        "        count = 0\n",
        "        for i in tqdm(range(0, indexer)):  # 훈련데이터\n",
        "            x_train.append(data_set[i][0])\n",
        "            y_train.append(data_set[i][1])  # 위에서 [img, label]로 append해서 이렇게 [i][0] , [i][1]\n",
        "\n",
        "            save_img_path = train_data_path + \"%d.jpg\" % count  # 이미지 경로\n",
        "            save_label_file = train_data_path + \"%d.txt\" % count  # 라벨(텍스트) 경로\n",
        "\n",
        "            cv2.imwrite(save_img_path, data_set[i][0])  # 이미지 저장\n",
        "            with open(save_label_file, 'w') as txt:  # 텍스트 저장 라벨링 한 숫자에 맞아떨어지게\n",
        "                txt.writelines(str(data_set[i][1]))\n",
        "            count += 1\n",
        "\n",
        "        count = 0  # 시험 데이터도 돌려야 해서 local변수 선언 해준 것이다. 위의 count와 영향 없게\n",
        "        for i in tqdm(range(indexer, len(data_set))):\n",
        "            x_test.append(data_set[i][0])\n",
        "            y_test.append(data_set[i][1])\n",
        "\n",
        "            save_img_path = test_data_path + \"%d.jpg\" % count  # 이미지 경로\n",
        "            save_label_file = test_data_path + \"%d.txt\" % count  # 라벨(텍스트) 경로\n",
        "\n",
        "            cv2.imwrite(save_img_path, data_set[i][0])  # 이미지 저장\n",
        "\n",
        "            with open(save_label_file, 'w') as txt:  # 텍스트 저장\n",
        "                txt.writelines(str(data_set[i][1]))\n",
        "            count += 1\n",
        "\n",
        "        return x_train, y_train, x_test, y_test, traffic_dict\n",
        "\n",
        "    elif save_flag == 1:  # 이미 파일이 준비되어 있는 경우\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        x_test = []\n",
        "        y_test = []\n",
        "\n",
        "        train_data_list = glob.glob(train_data_path + \"*.jpg\")  # 데이터가 이미 존재하니까. 이미지 불러옴\n",
        "        test_data_list = glob.glob(test_data_path + \"*.jpg\")\n",
        "\n",
        "\n",
        "        for i in tqdm(range(0, len(train_data_list))):  # 이미지 하나하나 for문돌리면서\n",
        "            file = train_data_list[i]  # i번째 이미지\n",
        "            img = cv2.imread(file)  # 읽고\n",
        "            label_path = file.replace(\".jpg\", \".txt\")  # 라벨 데이터 읽기 위해 path만듬\n",
        "            with open(label_path, 'r') as txt:  # 라벨 데이터 txt안에서 라벨 읽어옴\n",
        "                label = txt.readlines()\n",
        "            label = int(label[0])  # 각 이미지 별 라벨 값을 얻는다.\n",
        "            x_train.append(img)  # 이미지는 x에\n",
        "            y_train.append(label)  # 라벨은 y에 넣는다\n",
        "        # 테스트 데이터도 마찬가지\n",
        "        for i in tqdm(range(0, len(test_data_list))):\n",
        "            file = test_data_list[i]\n",
        "            img = cv2.imread(file)\n",
        "            label_path = file.replace(\".jpg\", \".txt\")\n",
        "            with open(label_path, 'r') as txt:\n",
        "                label = txt.readlines()\n",
        "            label = int(label[0])\n",
        "            x_test.append(img)\n",
        "            y_test.append(label)\n",
        "        # 라벨 이름을 불러오기 위해 만들어둔 딕셔너리에서 찾아오는 방법\n",
        "        with open(traffic_name_txt, 'r') as txt:\n",
        "            label = txt.readlines()\n",
        "            traffic_dict = label[0].split(\"~~\")  # 만든 파일이 모두 띄어쓰기로 이루어진것\n",
        "\n",
        "        return x_train, y_train, x_test, y_test, traffic_dict\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/trafficsign/traffic_signs_classification/myData\"\n",
        "    model_size = 32\n",
        "    x_train, y_train, x_test, y_test, traffic_dict = traffic_data_gen(data_path, model_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 58511/58511 [00:04<00:00, 11783.64it/s]\n",
            "100%|██████████| 14628/14628 [00:01<00:00, 11368.71it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYl270VH_Ok-",
        "outputId": "e52e18f2-6aec-4053-ce8b-4a2e73db27fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorflow==1.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1MB 71kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.10.0)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 66.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.18.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.33.1)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 72.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (1.0.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0) (50.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (3.3.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=da3a9bdeabc4f37bd3ac0d7a53c1170b79d325d031ec49d26fdf34e21f2e359c\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XnNtQo4_OnP",
        "outputId": "0417e605-4011-464c-a351-c2532e221ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install keras==2.1.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 7.1MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.4.1)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U71KZxoo_OpX",
        "outputId": "6aac8c57-2c20-484d-e4c7-dcebd5960c02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGWzXlSo_Or8",
        "outputId": "db167c62-a8af-4f21-9055-3ee6eb141d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.8.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3xtcvwC_OuM",
        "outputId": "8da97628-2ae2-487a-f640-0d7dfb32491c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_1\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 43)\n",
        "y_test = keras.utils.to_categorical(y_test, 43)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(16, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(128, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "x= keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(43)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(58511, 32, 32, 3) (58511, 43) (14628, 32, 32, 3) (14628, 43)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 128)         18560     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 43)                2795      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 43)                0         \n",
            "=================================================================\n",
            "Total params: 55,663\n",
            "Trainable params: 55,663\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 58511 samples, validate on 14628 samples\n",
            "Epoch 1/10\n",
            "58511/58511 [==============================] - 41s 704us/step - loss: 1.6198 - acc: 0.5520 - val_loss: 0.6754 - val_acc: 0.8045\n",
            "Epoch 2/10\n",
            "58511/58511 [==============================] - 41s 708us/step - loss: 0.4797 - acc: 0.8583 - val_loss: 0.3630 - val_acc: 0.8892\n",
            "Epoch 3/10\n",
            "58511/58511 [==============================] - 40s 690us/step - loss: 0.2886 - acc: 0.9134 - val_loss: 0.2704 - val_acc: 0.9193\n",
            "Epoch 4/10\n",
            "58511/58511 [==============================] - 41s 701us/step - loss: 0.2128 - acc: 0.9370 - val_loss: 0.2037 - val_acc: 0.9414\n",
            "Epoch 5/10\n",
            "58511/58511 [==============================] - 41s 700us/step - loss: 0.1611 - acc: 0.9512 - val_loss: 0.1712 - val_acc: 0.9470\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_1-00005.h5\n",
            "Epoch 6/10\n",
            "58511/58511 [==============================] - 41s 703us/step - loss: 0.1353 - acc: 0.9581 - val_loss: 0.1402 - val_acc: 0.9569\n",
            "Epoch 7/10\n",
            "58511/58511 [==============================] - 42s 715us/step - loss: 0.1117 - acc: 0.9652 - val_loss: 0.1432 - val_acc: 0.9569\n",
            "Epoch 8/10\n",
            "58511/58511 [==============================] - 41s 704us/step - loss: 0.1034 - acc: 0.9682 - val_loss: 0.1529 - val_acc: 0.9560\n",
            "Epoch 9/10\n",
            "58511/58511 [==============================] - 41s 705us/step - loss: 0.0845 - acc: 0.9736 - val_loss: 0.1289 - val_acc: 0.9642\n",
            "Epoch 10/10\n",
            "58511/58511 [==============================] - 41s 708us/step - loss: 0.0814 - acc: 0.9743 - val_loss: 0.1104 - val_acc: 0.9678\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_1-00010.h5\n",
            "Acc: 96.78014766201805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNnAVfH6_Owt",
        "outputId": "deb487a7-3adf-4715-c2a1-069527b8e917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_2\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 43)\n",
        "y_test = keras.utils.to_categorical(y_test, 43)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(16, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "x= keras.layers.Dense(64)(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(43)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(58511, 32, 32, 3) (58511, 43) (14628, 32, 32, 3) (14628, 43)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 16)          1168      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 43)                2795      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 43)                0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 58511 samples, validate on 14628 samples\n",
            "Epoch 1/10\n",
            "58511/58511 [==============================] - 40s 682us/step - loss: 1.8522 - acc: 0.4948 - val_loss: 0.8157 - val_acc: 0.7581\n",
            "Epoch 2/10\n",
            "58511/58511 [==============================] - 40s 682us/step - loss: 0.6292 - acc: 0.8166 - val_loss: 0.4755 - val_acc: 0.8570\n",
            "Epoch 3/10\n",
            "58511/58511 [==============================] - 39s 674us/step - loss: 0.4056 - acc: 0.8847 - val_loss: 0.3160 - val_acc: 0.9075\n",
            "Epoch 4/10\n",
            "58511/58511 [==============================] - 40s 680us/step - loss: 0.2971 - acc: 0.9143 - val_loss: 0.2747 - val_acc: 0.9196\n",
            "Epoch 5/10\n",
            "58511/58511 [==============================] - 40s 688us/step - loss: 0.2370 - acc: 0.9308 - val_loss: 0.2656 - val_acc: 0.9198\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_2-00005.h5\n",
            "Epoch 6/10\n",
            "58511/58511 [==============================] - 40s 680us/step - loss: 0.1944 - acc: 0.9439 - val_loss: 0.2159 - val_acc: 0.9373\n",
            "Epoch 7/10\n",
            "58511/58511 [==============================] - 39s 671us/step - loss: 0.1684 - acc: 0.9495 - val_loss: 0.1848 - val_acc: 0.9472\n",
            "Epoch 8/10\n",
            "58511/58511 [==============================] - 40s 685us/step - loss: 0.1472 - acc: 0.9571 - val_loss: 0.1509 - val_acc: 0.9569\n",
            "Epoch 9/10\n",
            "58511/58511 [==============================] - 39s 672us/step - loss: 0.1286 - acc: 0.9628 - val_loss: 0.1478 - val_acc: 0.9571\n",
            "Epoch 10/10\n",
            "58511/58511 [==============================] - 40s 676us/step - loss: 0.1154 - acc: 0.9669 - val_loss: 0.1439 - val_acc: 0.9592\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_2-00010.h5\n",
            "Acc: 95.91878589007383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYjihnrH_Ozo",
        "outputId": "65f917cf-c0a1-4426-beda-4953d989b84f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_3\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 43)\n",
        "y_test = keras.utils.to_categorical(y_test, 43)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Conv2D(8, kernel_size=3,padding=\"same\")(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(43)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(58511, 32, 32, 3) (58511, 43) (14628, 32, 32, 3) (14628, 43)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 8)         224       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 43)                22059     \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 43)                0         \n",
            "=================================================================\n",
            "Total params: 22,367\n",
            "Trainable params: 22,367\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 58511 samples, validate on 14628 samples\n",
            "Epoch 1/10\n",
            "58511/58511 [==============================] - 37s 630us/step - loss: 1.4845 - acc: 0.6147 - val_loss: 0.6429 - val_acc: 0.8303\n",
            "Epoch 2/10\n",
            "58511/58511 [==============================] - 36s 622us/step - loss: 0.4598 - acc: 0.8814 - val_loss: 0.3527 - val_acc: 0.9074\n",
            "Epoch 3/10\n",
            "58511/58511 [==============================] - 37s 626us/step - loss: 0.2891 - acc: 0.9254 - val_loss: 0.2433 - val_acc: 0.9392\n",
            "Epoch 4/10\n",
            "58511/58511 [==============================] - 37s 639us/step - loss: 0.2132 - acc: 0.9451 - val_loss: 0.2053 - val_acc: 0.9467\n",
            "Epoch 5/10\n",
            "58511/58511 [==============================] - 37s 633us/step - loss: 0.1670 - acc: 0.9561 - val_loss: 0.1874 - val_acc: 0.9514\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_3-00005.h5\n",
            "Epoch 6/10\n",
            "58511/58511 [==============================] - 37s 628us/step - loss: 0.1437 - acc: 0.9614 - val_loss: 0.1587 - val_acc: 0.9619\n",
            "Epoch 7/10\n",
            "58511/58511 [==============================] - 36s 624us/step - loss: 0.1195 - acc: 0.9690 - val_loss: 0.1669 - val_acc: 0.9558\n",
            "Epoch 8/10\n",
            "58511/58511 [==============================] - 36s 623us/step - loss: 0.1065 - acc: 0.9710 - val_loss: 0.1412 - val_acc: 0.9641\n",
            "Epoch 9/10\n",
            "58511/58511 [==============================] - 37s 625us/step - loss: 0.0959 - acc: 0.9734 - val_loss: 0.1672 - val_acc: 0.9547\n",
            "Epoch 10/10\n",
            "58511/58511 [==============================] - 36s 616us/step - loss: 0.0864 - acc: 0.9764 - val_loss: 0.1211 - val_acc: 0.9689\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_3-00010.h5\n",
            "Acc: 96.88952693464589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ZT4Fsd_O1_",
        "outputId": "8fa5d590-5d6e-4fc6-b339-c604b608bf45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "m_name = \"traffic_4\"\n",
        "lr = 0.03\n",
        "epochs=10\n",
        "bath_size = 32\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = x_train.astype('float32')\n",
        "X_test = x_test.astype('float32')\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 43)\n",
        "y_test = keras.utils.to_categorical(y_test, 43)\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = keras.layers.Conv2D(3, kernel_size=3,padding=\"same\")(inputs)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x= keras.layers.Dense(43)(x)\n",
        "\n",
        "out = keras.layers.Activation(\"softmax\")(x)\n",
        "model = keras.models.Model(inputs, out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "if not os.path.isdir('./saved_models'):\n",
        "    os.makedirs('./saved_models')\n",
        "if not os.path.isdir('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "log_dir = os.path.join(\n",
        "    \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
        "                                               patience=20,\n",
        "                                               verbose=1, factor=0.5),\n",
        "             keras.callbacks.ModelCheckpoint(filepath='./saved_models/'+m_name+'-{epoch:05d}.h5',\n",
        "                                             verbose=1,\n",
        "                                             period=5),\n",
        "             keras.callbacks.TensorBoard(log_dir),\n",
        "             keras.callbacks.EarlyStopping(monitor='loss', patience=25, verbose=1)]\n",
        "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=bath_size, callbacks=callbacks, verbose=1)\n",
        "\n",
        "scores = model.evaluate(x_test,y_test, verbose=2)\n",
        "print(\"Acc:\", scores[1]*100)\n",
        "model.save('./saved_models/'+m_name+'_final.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(58511, 32, 32, 3) (58511, 43) (14628, 32, 32, 3) (14628, 43)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 32, 32, 3)         84        \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 16, 16, 3)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 43)                33067     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 43)                0         \n",
            "=================================================================\n",
            "Total params: 33,151\n",
            "Trainable params: 33,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 58511 samples, validate on 14628 samples\n",
            "Epoch 1/10\n",
            "58511/58511 [==============================] - 29s 500us/step - loss: 1.5548 - acc: 0.6184 - val_loss: 0.7172 - val_acc: 0.8289\n",
            "Epoch 2/10\n",
            "58511/58511 [==============================] - 29s 499us/step - loss: 0.5414 - acc: 0.8721 - val_loss: 0.4344 - val_acc: 0.8947\n",
            "Epoch 3/10\n",
            "58511/58511 [==============================] - 29s 498us/step - loss: 0.3733 - acc: 0.9108 - val_loss: 0.3432 - val_acc: 0.9181\n",
            "Epoch 4/10\n",
            "58511/58511 [==============================] - 28s 487us/step - loss: 0.2950 - acc: 0.9290 - val_loss: 0.2861 - val_acc: 0.9315\n",
            "Epoch 5/10\n",
            "58511/58511 [==============================] - 28s 481us/step - loss: 0.2466 - acc: 0.9413 - val_loss: 0.2586 - val_acc: 0.9359\n",
            "\n",
            "Epoch 00005: saving model to ./saved_models/traffic_4-00005.h5\n",
            "Epoch 6/10\n",
            "58511/58511 [==============================] - 29s 501us/step - loss: 0.2119 - acc: 0.9483 - val_loss: 0.2347 - val_acc: 0.9398\n",
            "Epoch 7/10\n",
            "58511/58511 [==============================] - 28s 485us/step - loss: 0.1909 - acc: 0.9533 - val_loss: 0.2075 - val_acc: 0.9467\n",
            "Epoch 8/10\n",
            "58511/58511 [==============================] - 28s 484us/step - loss: 0.1668 - acc: 0.9592 - val_loss: 0.1927 - val_acc: 0.9517\n",
            "Epoch 9/10\n",
            "58511/58511 [==============================] - 28s 484us/step - loss: 0.1490 - acc: 0.9637 - val_loss: 0.1828 - val_acc: 0.9545\n",
            "Epoch 10/10\n",
            "58511/58511 [==============================] - 28s 486us/step - loss: 0.1345 - acc: 0.9673 - val_loss: 0.1808 - val_acc: 0.9554\n",
            "\n",
            "Epoch 00010: saving model to ./saved_models/traffic_4-00010.h5\n",
            "Acc: 95.5359584358764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P9bvxuH_O4I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}